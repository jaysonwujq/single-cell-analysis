＃将２０个ｓａｍｐｌｅ文件进行拆分

1. Go to fluidigm.com/c1openapp/scripthub and navigate to the main mRNA Seq HT v2　script page.

C1_mRNA_Seq_HT_Demultiplex_Script.zip

2. mkdir input && output

3. name 

COL01_ROW01_R1.fastq
COL01_ROW01_R2.fastq
......

perl mRNASeqHT_demultiplex.pl –i input –o output

拆分数据２０ｓａｍｐｌｅ　到　８００个数据

###文件需要解压成fastq文件，切mRNASeq*.pl 大小为11.4kb，而输入 -i 后用tab建自动获取文件夹无效。。。
##用fastqc去质检测序质量
发现周霞，总体来说还好！
发现SW的：R1质量一般，R2质量较好！
##13:00 start

4.去ｐｏｌｙＡ的ｐｅｒｌ

循环如下
for i in *fastq
do
echo $i
perl polyA.pl $i ./f/$i"_f.fastq" ./b/$i"_bad.fastq" 6 3 
done

５．比对！！！（STAR + featurecounts）
#帮助文档太长，需要查询帮助
STAR -h | grep "sjdb"
##用STAR进行比对分析

#1. 需要准备文件: 
基因组文件:.fa()
index文件：需要利用基因组文件自己构建；

#1. Mapping to the reference
mkdir $genomeDir
STAR --runMode genomeGenerate --genomeDir ./genomeDir --genomeFastaFiles ./***.fa  --runThreadN 200

##將R1的文件移除，僅僅比對R2，加快時間。R1僅用來拆分細胞；R2用來定義mRNA分子的表達；
#1-pass STAR
STAR --genomeDir ./genomeDir --readFilesIn 1.fq --runThreadN 200
对比如是多个文件需写循环进行。。。
循环代码如下，放在了ｓｈ中（１．sh代码）：
for i in *.fastq
do
echo $i
STAR --genomeDir ./genomeDir --readFilesIn $i --outFileNamePrefix "$i" --runThreadN 200
done

＃构建ｉｎｄｅｘ２
mkdir $genomeDir2
STAR --runMode genomeGenerate --genomeDir ./genomeDir2 
--genomeFastaFiles Homo_sapiens.GRCh38.dna.primary_assembly.fa 
--sjdbFileChrStartEnd ./SJ --sjdbOverhang 75 --runThreadN 200
＃./SJ 中存放splice junction information contained in the file SJ.out.tab from the first pass


#2-pass STAR　alin
STAR --genomeDir ./genomeDir2 --readFilesIn 1.fq --runThreadN <n>

＃此外还是需要循环处理文件，在ｓｔａｒ２中输入如下代码，利用新的刚刚生成ｉｎｄｅｘ的文件夹　ｇｅｎｏｍｅＤｉｒ２和ｏｕｔｐｕｔ２
for i in *.fastq
do
echo $i
STAR --genomeDir ../genomeDir2 --readFilesIn $i --outFileNamePrefix ../output2/"$i" --runThreadN 500
done

＃＃利用ｂａｓｈ　ｓｔａｒ２　


＃＃利用ｆｅａｔｕｒｅＣｏｕｎｔｓ统计原始ｃｏｕｎｔｓ
##将所有ｓａｍ文件转移至另一个文件夹，如ｃｏｕｎｔｓ，在ｆｅａｔｕｒｅ.sh中写入如下代码：
for i in *sam
do
echo $i
featureCounts -t exon -g gene_name -a Homo_sapiens.GRCh38.86.gtf -o ./output/"$i" $i
done


＃＃在利用ｆｅａｔｕｒｅＣｏｕｎｔｓ 获取raw counts后需要对批量文件进行合并。。。

＃合并文件前，可能需要对文件进行预处理，如删除每个文件的第一行，具体如下：

for i in *sam
do
echo $i
sed -i '1d' $i
done

将８００个文件
awk '{print FILENAME"\t"$0}' * |grep -v Geneid >tmp.txt

＃１　合并后,在Ｒ中还需取文件的子集进行后续分析
a=read.table('tmp.txt',sep = '\t',stringsAsFactors = F)
b<-a[,c(1,2,8)]

library(reshape2)
fpkm <- dcast(b, formula = V2~V1)

row.names(fpkm)<-fpkm$V2
fpkm<-fpkm[,-1]
names(fpkm)<-unlist(lapply(names(fpkm),function(x){tmp<-strsplit(x,'_R2')[[1]][1]})) 

＃＃＃以上为最新版本ｓｉｎｇｌｅ　ｃｅｌｌ　Ｃ１　ｓｅｑｕｅｎｃｅ。。。


代码如下：

rm(list = ls())
 
## 首先在GSE48213_RAW目录里面生成tmp.txt文件
awk '{print FILENAME"\t"$0}' * |grep -v Gene_ID >tmp.txt
 
## awk 为linux 下批处理命令，输出当前文件名， $0表示整个记录--awk '{print FILENAME"\t"$0}' * |grep -v EnsEMBL_Gene_ID >tmp.txt
 
##去除表头---grep -v EnsEMBL_Gene_ID
##赋值给tmp.txt
 
## 然后把tmp.txt导入R语言里面用reshape2处理即可！
setwd('C:/Users/Administrator/Desktop/learn/lecture4/新建文件夹/tmp')
a=read.table('tmp.txt',sep = '\t',stringsAsFactors = F)
 
library(reshape2)
fpkm <- dcast(a,formula = V2~V1)
??dcast
 
row.names(fpkm)<-fpkm$V2
fpkm<-fpkm[,-1]

## 改名
names(fpkm)<-unlist(lapply(names(fpkm),function(x){
  tmp<-strsplit(x,'_')[[1]][2]  
  tmp<-strsplit(tmp,'\\.')[[1]][1]
}))  ##体会[1]与[[1]]的区别
 
write.csv(fpkm,"56sample fpkm.csv")

##读取ＣＤ１９＋数据
d<-fpkm[rownames(fpkm) %in% "CD19",]
e<-as.data.frame(t(d))
e[which(e$CD19>0),]
length(e[which(e$CD19>0),])

＃＃简单的代码
length(which(fpkm[rownames(fpkm) %in% "CD19",]>0))

合并两个Ｃ１文件
all<-merge(zhouxia,lou,by="row.names",all=T)


